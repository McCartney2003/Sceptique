#Text Extraction



!sudo apt install tesseract-ocr

!pip install pytesseract

import pytesseract

import shutil

import os

import random

try:

  from PIL import Image

except ImportError:

  import Image

image_path_in_colab='/content/drive/MyDrive/image_test.jpeg'

extractedInformation = pytesseract.image_to_string(Image.open(image_path_in_colab))

print(extractedInformation)



##Summarizer

!pip install transformers
!pip install scipy

!pip install torch
!pip install transformers

import torch
from transformers import AutoTokenizer, AutoModelWithLMHead

#Downloading tokenizer and model:

tokenizer = AutoTokenizer.from_pretrained('t5-base')
model = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)

!pip install transformers
!pip install scipy

!pip install torch
!pip install transformers

import torch
from transformers import AutoTokenizer, AutoModelWithLMHead

#Downloading tokenizer and model:

tokenizer = AutoTokenizer.from_pretrained('t5-base')
model = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)

print("Enter Minimum Length of Summary")
min=int(input())

print("Enter Maximum Length of Summary")
maxi=int(input())

sequence = extractedInformation
inputs = tokenizer.encode('summarize: '+ sequence, return_tensors='pt', max_length = 512, truncation = True)
outputs = model.generate(inputs,min_length = min, max_length = maxi,length_penalty = 5, num_beams=2)
summary = tokenizer.decode(outputs[0])
length = len(summary)
print(summary[6:length-4])

##Summarized Audio

!pip install gTTS

text_to_speech=summary[6:length-4]

from gtts import gTTS
from IPython.display import Audio
tts = gTTS(text_to_speech)
tts.save('1.wav')
sound_file = '1.wav'
Audio(sound_file, autoplay=True)

##Fake News 

import re
from nltk.corpus import stopwords
from nltk import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.linear_model import LogisticRegression


import nltk

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

nltk.download('averaged_perceptron_tagger')

lemmatizer = WordNetLemmatizer()
from nltk.corpus import wordnet
# Define function to lemmatize each word with its POS tag
 
# POS_TAGGER_FUNCTION : TYPE 1
def pos_tagger(nltk_tag):
    if nltk_tag.startswith('J'):
        return wordnet.ADJ
    elif nltk_tag.startswith('V'):
        return wordnet.VERB
    elif nltk_tag.startswith('N'):
        return wordnet.NOUN
    elif nltk_tag.startswith('R'):
        return wordnet.ADV
    else:         
        return None


pos_tagged = nltk.pos_tag(nltk.word_tokenize(extractedInformation)) 
wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))

lemmatized_sentence = []
for word, tag in wordnet_tagged:
    if tag is None:
        # if there is no available tag, append the token as is
        lemmatized_sentence.append(word)
    else:       
        # else use the tag to lemmatize the token
        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))
lemmatized_sentence = " ".join(lemmatized_sentence)

lemmatized_sentence

import string
def review_cleaning(text):
    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub("'",'',text)
    text = re.sub('\w*\d\w*', '', text)
    return text

article=review_cleaning(lemmatized_sentence)
article

import re
pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('english')) + r')\b\s*')
text = pattern.sub('', article)

text

#Removing additional whitespaces 
article = re.sub('\s+', ' ', text)

article

from sklearn.feature_extraction.text import TfidfVectorizer
import pickle
vectorizer = pickle.load(open('/content/drive/MyDrive/tfidf.pickle','rb'))
X_test = vectorizer.transform([article])

model=pickle.load(open('/content/drive/MyDrive/fake_news_fulltrained_logistic_2.unknown','rb'))

prediction = model.predict(X_test)
print(prediction)

if (prediction[0]==0):
  print('The news is Real')
else:
  print('The news is Fake')

##Sentiment Analysis

import nltk
nltk.download('punkt')

import torch
!pip install flair
import flair

from flair.models import TextClassifier
from flair.data import Sentence
from segtok.segmenter import split_single
classifier = TextClassifier.load('en-sentiment')

def score_flair(text):
  str(text)
  sentence = Sentence(text)
  classifier.predict(sentence)
  score = sentence.labels[0].score
  value = sentence.labels[0].value
  return score, value

print("Sentiment Analysis of the given article is ", score_flair(article)[1])


